# AI QuickFix

Use open-source AI tools to find one-click 'lightbulb menu' solutions to problems in your code.

## Features

- Integrates with your code editor's diagnostics to identify problems
- Uses open source AI models to suggest fixes
- Provides one-click solutions through VS Code's "lightbulb" menu
- Works offline with locally hosted models
- Supports multiple programming languages
- Configurable to use different open source models

## Requirements

- A compatible open source model server (Ollama, LocalAI, etc.)
- A code-specialized model (CodeLlama, WizardCoder, StarCoder, etc.)

## Extension Settings

* `haselerdev.aiquickfix.modelEndpoint`: Endpoint for the local or self-hosted model API
* `haselerdev.aiquickfix.modelName`: Which model to use for AI problem solving
* `haselerdev.aiquickfix.systemPrompt`: The system prompt giving the AI instructions
* `haselerdev.aiquickfix.maxTokens`: Maximum number of tokens to generate
* `haselerdev.aiquickfix.temperature`: Temperature for model generation (0.0-1.0)
* `haselerdev.aiquickfix.backgroundFix.enabled`: Enable automatic background fixing of code issues

## How to Use

1. Set up a local or self-hosted model server (see below)
2. Install this extension
3. Configure the extension to connect to your model server
4. When you see a diagnostic issue (red squiggly line) in your code, click on the lightbulb icon to see AI-suggested fixes

## Setting Up a Model Server

We recommend using [Ollama](https://ollama.ai/) which makes it simple to run open source models locally:

```bash
# Install Ollama (see https://ollama.ai/ for platform-specific instructions)

# Pull a code-specialized model
ollama pull codeellama:7b

# Start the Ollama server (runs on http://localhost:11434 by default)
ollama serve
```

## License

[MIT License](LICENSE)
